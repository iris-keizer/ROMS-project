{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aa2d50-5192-4778-939b-0a7de3d5644b",
   "metadata": {},
   "source": [
    "# Notebook to create the lateral forcing myself\n",
    "\n",
    "\n",
    "\n",
    "#### xESMF\n",
    "Use xesmf (https://pangeo-xesmf.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "Use similar method as https://github.com/trondkr/model2roms\n",
    "\n",
    "Use a 1/4째 NWES grid provided by Dewi: 'NorthSea4_smooth01_sponge_nudg.nc'\n",
    "\n",
    "\n",
    "\n",
    "#### Regridding steps\n",
    "First regrid all variables (zos, uo, vo, thetao, so) from cartesian (lat, lon) to RHO points (eta_rho, xi_rho) for each Z-level. \n",
    "Zos only has surface values and therefore only 2D coordinates. Thereafter, the velocities uo, vo have to be interpolated to respectively (eta_u, xi_u) and (eta_v, xi_v) When the horizontal interpolation is done for all variables, the Z-levels are interpolated to sigma levels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Use masks\n",
    "xESMF treats NaNs like regular values hence potentially resulting in missing values bleeding into the regridded field and creating insconsistencies in the resulting masked array. To overcome this issue, we can use explicit masking of the source and target grids.\n",
    "\n",
    "\n",
    "Be carefull that the mask is different for different depth levels and therefore the horizontal regridding should be done for each depth level seperately. Trond does not take into account depth varying mask when regridding. Also, the roms grid does not include a mask that varies for depth levels.\n",
    "\n",
    "\n",
    "\n",
    "Remember to change NaN values to fillvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9b86d-3547-45bf-994a-edf6ff8c90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32f6e4-7482-43e1-af79-486da047be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import grid\n",
    "\n",
    "grid = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/data/grid/NorthSea4_smooth01_sponge_nudg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78bcda-eca2-4748-9e67-9378fa210094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "glorys_test = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/data/GLORYS12V1/processed/glorys_test.nc')\n",
    "ora_test = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/data/ORA20C/forcing input/ora20_test.nc')\n",
    "\n",
    "\n",
    "data = ora_test\n",
    "data_type = 'ORA20C' # Choose 'GLORYS' or 'ORA20C'\n",
    "\n",
    "# Sort coordinates in increasing order\n",
    "data = data.sortby(['depth', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf8c042-1528-432f-92b1-bbf2d560aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tim lateral forcing files\n",
    "ini_tim = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/timnorthsea8/NS8_ForcingSmooth01/NS8_init_GLORYS_19930115_to_20021231.nc')\n",
    "clim_tim = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/timnorthsea8/NS8_ForcingSmooth01/NS8_clim_GLORYS_19930115_to_20021231.nc')\n",
    "bry_tim = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/timnorthsea8/NS8_ForcingSmooth01/NS8_bry_GLORYS_19930115_to_20021231.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a77f86-1767-41f0-b0f8-5b82b8adf487",
   "metadata": {},
   "source": [
    "### Show the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d70714-b700-45e7-a4e3-06fe839bd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.lat_rho.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0855c51-cdd8-4695-b967-4e7873219afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.lon_rho.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc95419-c169-4b41-9754-4d8e330b5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948cdc4-6f50-464a-8c8e-8a327870a5f5",
   "metadata": {},
   "source": [
    "## Perform the horizontal interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c6ac6-2da0-45fb-a6dc-ae241dcdc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output grids\n",
    "\n",
    "output_grid_rho = xr.Dataset(data_vars=dict(mask=([\"eta_rho\", \"xi_rho\"], grid.mask_rho.values),), \n",
    "                           coords=dict(eta_rho=([\"eta_rho\"], grid.eta_rho.values),\n",
    "                                       xi_rho=([\"xi_rho\"], grid.xi_rho.values),\n",
    "                                       lat=([\"eta_rho\", \"xi_rho\"], grid.lat_rho.values),\n",
    "                                       lon=([\"eta_rho\", \"xi_rho\"], grid.lon_rho.values),\n",
    "                                      ))\n",
    "\n",
    "output_grid_u = xr.Dataset(data_vars=dict(mask=([\"eta_u\", \"xi_u\"], grid.mask_u.values),), \n",
    "                           coords=dict(eta_u=([\"eta_u\"], grid.eta_u.values),\n",
    "                                       xi_u=([\"xi_u\"], grid.xi_u.values),\n",
    "                                       lat=([\"eta_u\", \"xi_u\"], grid.lat_u.values),\n",
    "                                       lon=([\"eta_u\", \"xi_u\"], grid.lon_u.values),\n",
    "                                      ))\n",
    "\n",
    "\n",
    "output_grid_v = xr.Dataset(data_vars=dict(mask=([\"eta_v\", \"xi_v\"], grid.mask_v.values),), \n",
    "                           coords=dict(eta_u=([\"eta_v\"], grid.eta_v.values),\n",
    "                                       xi_u=([\"xi_v\"], grid.xi_v.values),\n",
    "                                       lat=([\"eta_v\", \"xi_v\"], grid.lat_v.values),\n",
    "                                       lon=([\"eta_v\", \"xi_v\"], grid.lon_v.values),\n",
    "                                      ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_masks(input_grid, output_grid, coord = 'RHO'):\n",
    "    ''' \n",
    "    Function to plot the mask used for the reanalysis data and the ROMS grid.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(ncols = 2, figsize = (8,3))\n",
    "    \n",
    "    input_grid.mask.plot(ax = axes[0], cmap = 'binary_r', )\n",
    "    output_grid.mask.plot(ax = axes[1], cmap = 'binary_r')\n",
    "    \n",
    "    axes[0].set_title(f'Mask of reanalysis data for {coord}-coordinates')\n",
    "    axes[1].set_title(f'Mask of ROMS grid for {coord}-coordinates')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    \n",
    "def horizontal_interp1D(input_data, var):\n",
    "    ''' \n",
    "    Function to perform the horizontal regridding per depth level\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if var == 'thetao':\n",
    "        \n",
    "        output_grid = output_grid_rho\n",
    "    \n",
    "    elif var == 'uo':\n",
    "        \n",
    "        output_grid = output_grid_u\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        output_grid = output_grid_v\n",
    "    \n",
    "  \n",
    "    \n",
    "    # Prepare the input grid\n",
    "    input_grid = xr.Dataset(data_vars=dict(mask=([\"lat\", \"lon\"], np.nan_to_num(input_data[0, :, :] / input_data[0, :, :])),), \n",
    "                           coords=dict(time=([\"time\"], data.time.values),\n",
    "                                       lat=([\"lat\"], data.latitude.values),\n",
    "                                       lon=([\"lon\"], data.longitude.values),\n",
    "                                      ))\n",
    "    \n",
    "    \n",
    "    # Make arrays C_CONTIGUOUS\n",
    "    input_grid = input_grid.astype(dtype = 'int64', order = 'C')\n",
    "    output_grid = output_grid.astype(dtype = 'float64', order = 'C')\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Build regridder\n",
    "    regridder = xe.Regridder(input_grid, output_grid, \"bilinear\", extrap_method= 'nearest_s2d')\n",
    "\n",
    "\n",
    "    # Apply to data\n",
    "    output_data = regridder(input_data)\n",
    "    \n",
    "    \n",
    "    return output_data\n",
    "    \n",
    "    \n",
    "    \n",
    "def horizontal_interp(input_data, grid, map_masks = False):\n",
    "    ''' Function to perform the horizontal interpolation from latitude, longitude coordinates to (eta_rho, xi_rho), (eta_u, xi_u) or (eta_v, xi_v). \n",
    "        The horizontal interpolation is performed as a linear interpolation.\n",
    "        \n",
    "        The glorys data with a resolution of 1/12째 or the ora data with a resolution of 1.0째 is regridded on the ROMS grid that has a resolution of 1/4째.\n",
    "        \n",
    "        '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Some data preparations\n",
    "    \n",
    "    # Rename coordinates to 'lat' and 'lon'\n",
    "    input_data = input_data.rename({'latitude':'lat', 'longitude':'lon', 'time':'time'})\n",
    "\n",
    "    # Sort coordinates in increasing order\n",
    "    input_data = input_data.sortby(['lat', 'lon'])\n",
    "\n",
    "    # Transpose dimensions\n",
    "    input_data = input_data.transpose('depth', 'time', 'lat', 'lon')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Regrid Sea level to 'rho'-coordinates\n",
    "    print(f'===================== Start horizontal interpolation of Sea Level (Duration: {round(time.time() - start_time,2)} seconds) ========================================================')\n",
    "    \n",
    "    \n",
    "    data_rho = input_data.drop(['uo', 'vo', 'thetao', 'so'])\n",
    "    \n",
    "    \n",
    "    # Prepare the output grid\n",
    "    output_grid = xr.Dataset(data_vars=dict(mask=([\"eta_rho\", \"xi_rho\"], grid.mask_rho.values),), \n",
    "                           coords=dict(eta_rho=([\"eta_rho\"], grid.eta_rho.values),\n",
    "                                       xi_rho=([\"xi_rho\"], grid.xi_rho.values),\n",
    "                                       lat=([\"eta_rho\", \"xi_rho\"], grid.lat_rho.values),\n",
    "                                       lon=([\"eta_rho\", \"xi_rho\"], grid.lon_rho.values),\n",
    "                                      ))\n",
    "    \n",
    "  \n",
    "    # Prepare the input grid\n",
    "    input_grid = xr.Dataset(data_vars=dict(mask=([\"lat\", \"lon\"], xr.where(~np.isnan(data_rho.zos.isel(time=0)), 1, 0).values),), \n",
    "                           coords=dict(time=([\"time\"], data_rho.time.values),\n",
    "                                       lat=([\"lat\"], data_rho.lat.values),\n",
    "                                       lon=([\"lon\"], data_rho.lon.values),\n",
    "                                      ))\n",
    "\n",
    "\n",
    "    # Make arrays C_CONTIGUOUS\n",
    "    input_grid = input_grid.astype(dtype = 'int64', order = 'C')\n",
    "    output_grid = output_grid.astype(dtype = 'float64', order = 'C')\n",
    "    \n",
    "    if map_masks:\n",
    "        \n",
    "        plot_masks(input_grid, output_grid, 'RHO')\n",
    "        \n",
    "        \n",
    "    # Build regridder\n",
    "    regridder = xe.Regridder(input_grid, output_grid, \"bilinear\", extrap_method= 'nearest_s2d')\n",
    "\n",
    "\n",
    "    # Apply to data\n",
    "    output_data_rho_zos = regridder(data_rho)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Regrid Salinity and Temperature to 'rho'-coordinates\n",
    "    print(f'===================== Start horizontal interpolation of Salinity and Temperature (Duration: {round(time.time() - start_time,2)} seconds) =========================================')\n",
    "    \n",
    "    data_rho_ST = input_data.drop(['uo', 'vo', 'zos'])\n",
    "    \n",
    "    \n",
    "    # Perform regridding parallel\n",
    "    output_data_rho_ST = xr.apply_ufunc(horizontal_interp1D,\n",
    "                                       data_rho_ST, 'thetao',\n",
    "                                       input_core_dims=[['time', 'lat', 'lon'], []],\n",
    "                                       output_core_dims = [['time', 'eta_rho', 'xi_rho']],\n",
    "                                       dask = 'parallelized',\n",
    "                                       output_dtypes = [data_rho_ST.thetao.dtype],\n",
    "                                       vectorize = True)\n",
    "    \n",
    "    output_data_rho_ST = output_data_rho_ST.assign_coords(eta_rho = grid.eta_rho, xi_rho = grid.xi_rho)\n",
    "    \n",
    "    \n",
    "    # Regrid Zonal velocities to 'u'-coordinates\n",
    "    print(f'===================== Start horizontal interpolation of Zonal Velocity (Duration: {round(time.time() - start_time,2)} seconds) ====================================================')\n",
    "    \n",
    "    data_u = input_data.drop(['so', 'thetao', 'vo', 'zos'])\n",
    "    \n",
    "    \n",
    "    # Perform regridding parallel\n",
    "    output_data_u = xr.apply_ufunc(horizontal_interp1D,\n",
    "                                       data_u, 'uo',\n",
    "                                       input_core_dims=[['time', 'lat', 'lon'], []],\n",
    "                                       output_core_dims = [['time', 'eta_u', 'xi_u']],\n",
    "                                       dask = 'parallelized',\n",
    "                                       output_dtypes = [data_u.uo.dtype],\n",
    "                                       vectorize = True)\n",
    "    \n",
    "    \n",
    "    output_data_u = output_data_u.assign_coords(eta_u = grid.eta_u, xi_v = grid.xi_u)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Regrid Meridional velocities to 'v'-coordinates\n",
    "    print(f'===================== Start horizontal interpolation of Meridional Velocity (Duration: {round((time.time() - start_time),2)} seconds) =============================================')\n",
    "    \n",
    "    \n",
    "    data_v = input_data.drop(['so', 'thetao', 'uo', 'zos'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform regridding parallel\n",
    "    output_data_v = xr.apply_ufunc(horizontal_interp1D,\n",
    "                                       data_v, 'vo',\n",
    "                                       input_core_dims=[['time', 'lat', 'lon'], []],\n",
    "                                       output_core_dims = [['time', 'eta_v', 'xi_v']],\n",
    "                                       dask = 'parallelized',\n",
    "                                       output_dtypes = [data_v.vo.dtype],\n",
    "                                       vectorize = True)\n",
    "    \n",
    "    \n",
    "    output_data_v = output_data_v.assign_coords(eta_v = grid.eta_v, xi_v = grid.xi_v)\n",
    "    \n",
    "    \n",
    "    # Create one final dataset\n",
    "    print('===================== Finalised horizontal interpolation, creating final dataset ==============================================')\n",
    "    \n",
    "    \n",
    "    # Create one horizontally regridded file\n",
    "    output_data = output_data_rho_ST.copy()\n",
    "\n",
    "    # Add zos, uo and vo\n",
    "    output_data = output_data.assign(zos=output_data_rho_zos.zos)\n",
    "    output_data = output_data.assign(uo=output_data_u.uo)\n",
    "    output_data = output_data.assign(vo=output_data_v.vo)\n",
    "\n",
    "    \n",
    "    output_data.to_netcdf('/Users/iriskeizer/Documents/ROMS/data/lateral forcing/NorthSea4 ORA20C/data_hr_test.nc')\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ef022-8ecb-4cc2-a206-36373cbf6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hr = horizontal_interp(data, grid, map_masks = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402a100-8ef9-4282-aad5-9b40687ee540",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9754aa6-4074-423c-8c09-16df942da9c4",
   "metadata": {},
   "source": [
    "#### Check sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4518a-3b41-4a9b-ae49-a78d0eb40684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "fig, axes = plt.subplots(ncols = 3, figsize = (15,3.5))\n",
    "\n",
    "\n",
    "data.zos.isel(time=time_step).plot(ax = axes[0])\n",
    "data_hr.zos.isel(time=time_step).plot(ax = axes[1], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r')\n",
    "clim_tim.zeta.isel(ocean_time=time_step).plot(ax = axes[2], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r')\n",
    "\n",
    "axes[0].set_title(f'{data_type} reanalysis data (1.0째) \\n {data.time.isel(time=time_step).dt.date}')\n",
    "axes[1].set_title(f'Horizontally regridded data (1/4째)\\n {data_hr.time.isel(time=time_step).dt.date}')\n",
    "axes[2].set_title(f'Climatology forcing Tim (1/8째)\\n {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}')\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c820f-3f77-45b4-be48-c8535e756b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = -1\n",
    "\n",
    "\n",
    "# Create figures\n",
    "fig, axes = plt.subplots(ncols = 3, figsize = (15,3.5))\n",
    "\n",
    "\n",
    "data.zos.isel(time=time_step).plot(ax = axes[0])\n",
    "data_hr.zos.isel(time=time_step).plot(ax = axes[1], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r')\n",
    "clim_tim.zeta.isel(ocean_time=time_step).plot(ax = axes[2], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r')\n",
    "\n",
    "axes[0].set_title(f'{data_type} reanalysis data (1.0째) \\n {data.time.isel(time=time_step).dt.date}')\n",
    "axes[1].set_title(f'Horizontally regridded data (1/4째)\\n {data_hr.time.isel(time=time_step).dt.date}')\n",
    "axes[2].set_title(f'Climatology forcing Tim (1/8째)\\n {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}')\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12272376-ada8-4528-8da4-20c38ef4e4c2",
   "metadata": {},
   "source": [
    "The horizontal regridding of sea level seems to be succesfull!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e4486-afed-437c-8d4f-b27cf03468b1",
   "metadata": {},
   "source": [
    "### Check temperature and salinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6fa0d-2cac-48db-99bd-439f909161f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = data.depth.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Plot Temperature maps\n",
    "    \n",
    "    data.thetao.isel(time = time_step, depth = i).plot(ax = axes[i, 0], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'{data_type} reanalysis data  \\n time =  {data.time.isel(time=time_step).dt.date}\\n depth = {int(data.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f'depth = {int(data.depth.isel(depth = i))}m')\n",
    "    \n",
    "    data_hr.thetao.isel(time = time_step, depth = i).plot(ax = axes[i, 1], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Horizontally regridded data  \\n time =  {data_hr.time.isel(time=time_step).dt.date}\\n depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f'depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Salinity maps\n",
    "    \n",
    "    \n",
    "    data.so.isel(time = time_step, depth = i).plot(ax = axes[i, 2], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'{data_type} reanalysis data  \\n time =  {data.time.isel(time=time_step).dt.date}\\n depth = {int(data.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f'depth = {int(data.depth.isel(depth = i))}m')\n",
    "    \n",
    "    data_hr.so.isel(time = time_step, depth = i).plot(ax = axes[i, 3], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Horizontally regridded data  \\n time =  {data_hr.time.isel(time=time_step).dt.date}\\n depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f'depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919ca53-bbfa-4c9b-bbb0-54bbbac99034",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hr.thetao.isel(depth=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1383859-ce12-4aa3-a0ad-405262ec496b",
   "metadata": {},
   "source": [
    "The deepest values are all nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451eea12-80e8-40a3-8f26-c463ffb8742f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7119bd33-134d-4e39-9469-b3af1d29b02b",
   "metadata": {},
   "source": [
    "### Check zonal and meridional velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712284d-cf22-423c-8ea1-bb4b3b4d92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = data.depth.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Change facecolor\n",
    "    axes[i,0].set_facecolor('lightgray')\n",
    "    axes[i,1].set_facecolor('lightgray')\n",
    "    axes[i,2].set_facecolor('lightgray')\n",
    "    axes[i,3].set_facecolor('lightgray')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Temperature maps\n",
    "    \n",
    "    data.uo.isel(time = time_step, depth = i).plot(ax = axes[i, 0], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'{data_type} reanalysis data  \\n time =  {data.time.isel(time=time_step).dt.date}\\n depth = {int(data.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f'depth = {int(data.depth.isel(depth = i))}m')\n",
    "    \n",
    "    data_hr.uo.isel(time = time_step, depth = i).plot(ax = axes[i, 1], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Horizontally regridded data  \\n time =  {data_hr.time.isel(time=time_step).dt.date}\\n depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f'depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Salinity maps\n",
    "    \n",
    "    \n",
    "    data.vo.isel(time = time_step, depth = i).plot(ax = axes[i, 2], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'{data_type} reanalysis data  \\n time =  {data.time.isel(time=time_step).dt.date}\\n depth = {int(data.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f'depth = {int(data.depth.isel(depth = i))}m')\n",
    "    \n",
    "    data_hr.vo.isel(time = time_step, depth = i).plot(ax = axes[i, 3], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Horizontally regridded data  \\n time =  {data_hr.time.isel(time=time_step).dt.date}\\n depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f'depth = {int(data_hr.depth.isel(depth = i))}m')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84e12a-a6e4-436a-be51-da833c9093e5",
   "metadata": {},
   "source": [
    "The deepest values are only nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e44104-93ff-4789-8c1e-ab123400192c",
   "metadata": {},
   "source": [
    "## Apply La Place filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb60a2e8-5c10-4e80-9272-8c84aa360b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grdMODEL(data):\n",
    "    ''' Function to create the dataset grdMODEL '''\n",
    "    \n",
    "    grdMODEL = data.copy().drop(['thetao', 'so', 'uo', 'vo'])\n",
    "    \n",
    "    grdMODEL['lon'] = data.lon\n",
    "    grdMODEL['lat'] = data.lat\n",
    "    grdMODEL['h'] = data.depth\n",
    "    grdMODEL['nlevels'] = grdMODEL.h.size\n",
    "    \n",
    "    \n",
    "    grdMODEL['fillval'] = -32767   # Change for ORA-20C\n",
    "    grdMODEL['hc'] = None\n",
    "\n",
    "    # Create grid for ESMF interpolation, probably not needed for VT\n",
    "\n",
    "    grdMODEL['z_r'] = -grdMODEL.h\n",
    "\n",
    "    grdMODEL['grdType'] = 'regular'\n",
    "    grdMODEL['lonName'] = 'longitude'\n",
    "    grdMODEL['latName'] = 'latitude'\n",
    "    grdMODEL['depthName'] = 'depth'\n",
    "\n",
    "\n",
    "    grdMODEL['Lp'] = len(grdMODEL.lat[1,:])\n",
    "    grdMODEL['Mp'] = len(grdMODEL.lat[:,1])\n",
    "\n",
    "    grdMODEL['L'] = grdMODEL.Lp - 1\n",
    "    grdMODEL['M'] = grdMODEL.Mp - 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    return grdMODEL\n",
    "\n",
    "def create_grdROMS(grid):\n",
    "    ''' Function to create the dataset grdROMS '''\n",
    "    \n",
    "    # Create the dataset grdROMS\n",
    "\n",
    "    # Copy the roms grid\n",
    "    grdROMS = grid.copy()\n",
    "\n",
    "    # Drop unnecessary variables\n",
    "    grdROMS = grdROMS.drop(['tracer_NudgeCoef', 'diff_factor', 'visc_factor', 'hraw', 'f', 'spherical'])\n",
    "\n",
    "\n",
    "\n",
    "    # Add below variables to grdROMS\n",
    "    grdROMS['write_clim'] = True\n",
    "    grdROMS['write_bry'] = True\n",
    "    grdROMS['write_init'] = True\n",
    "    grdROMS['write_stations'] = False\n",
    "    grdROMS['lonname'] = 'lon_rho'\n",
    "    grdROMS['latname'] = 'lat_rho'\n",
    "    grdROMS['inittime'] = 0                    # Set initTime to 1 if you dont want the first time-step to be the initial field (no ubar and vbar if time=0)\n",
    "    grdROMS['ocean_time'] = 0\n",
    "    grdROMS['NT'] = 2\n",
    "    grdROMS['tracer'] = grdROMS.NT\n",
    "    grdROMS['time'] = 0                      \n",
    "    grdROMS['reftime'] = 0\n",
    "    grdROMS['grdtype'] = 'regular'\n",
    "\n",
    "    grdROMS['masked_h'] = grdROMS.h.where(grdROMS.h > 0, grdROMS.h, grdROMS.h.max())\n",
    "    grdROMS['hmin'] = grdROMS.masked_h.min()\n",
    "\n",
    "    grdROMS['vtransform'] = 2\n",
    "    grdROMS['vstretching'] = 4\n",
    "\n",
    "    grdROMS['nlevels'] = grdROMS.s_rho.size\n",
    "\n",
    "    grdROMS['zeta'] = (('eta_rho', 'xi_rho'), np.zeros(grdROMS.h.shape))\n",
    "\n",
    "    grdROMS['invpm'] = 1.0 / grdROMS.pm\n",
    "    grdROMS['invpn'] = 1.0 / grdROMS.pn\n",
    "\n",
    "    grdROMS['Lp'] = grdROMS.lat_rho[1,:].size     \n",
    "    grdROMS['Mp'] = grdROMS.lat_rho[:,1].size     \n",
    "\n",
    "    grdROMS['fillval'] = -9.99e33\n",
    "\n",
    "    grdROMS['eta_rho_'] = grdROMS.Mp\n",
    "    grdROMS['eta_u_'] = grdROMS.Mp\n",
    "    grdROMS['eta_v_'] = grdROMS.Mp - 1\n",
    "    grdROMS['eta_psi_'] = grdROMS.Mp - 1\n",
    "\n",
    "\n",
    "    grdROMS['xi_rho_'] = grdROMS.Lp\n",
    "    grdROMS['xi_u_'] = grdROMS.Lp - 1\n",
    "    grdROMS['xi_v_'] = grdROMS.Lp\n",
    "    grdROMS['xi_psi_'] = grdROMS.Lp - 1\n",
    "\n",
    "\n",
    "\n",
    "    # Obtain s_rho\n",
    "\n",
    "    c1 = 1.0\n",
    "    c2 = 2.0\n",
    "    p5 = 0.5\n",
    "\n",
    "    lev = np.arange(1, int(grdROMS.nlevels) + 1, 1)\n",
    "    ds = 1.0 / int(grdROMS.nlevels)\n",
    "\n",
    "\n",
    "    grdROMS['s_rho_'] = - c1 + (lev - p5) * ds\n",
    "\n",
    "\n",
    "    # Obtain s_w\n",
    "\n",
    "    lev = np.arange(0, int(grdROMS.nlevels), 1)\n",
    "    ds = 1.0 / (int(grdROMS.nlevels) - 1)\n",
    "\n",
    "\n",
    "    grdROMS['s_w_'] = - c1 + (lev - p5) * ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Obtain Cs_r\n",
    "\n",
    "    if (grdROMS.theta_s > 0):\n",
    "        Csur = (c1 - np.cosh(grdROMS.theta_s * grdROMS.s_rho)) / (np.cosh(grdROMS.theta_s) - c1)\n",
    "\n",
    "    else:\n",
    "        Csur = -grdROMS.s_rho**2\n",
    "\n",
    "    if (grdROMS.theta_b > 0):\n",
    "        Cbot = (np.exp(grdROMS.theta_b * Csur) - c1 ) / (c1 - np.exp(-grdROMS.theta_b))\n",
    "        grdROMS['Cs_r'] = Cbot\n",
    "    else:\n",
    "        grdROMS['Cs_r'] = Csur     \n",
    "\n",
    "\n",
    "\n",
    "    # Obtain Cs_w\n",
    "\n",
    "    if (grdROMS.theta_s > 0):\n",
    "        Csur = (c1 - np.cosh(grdROMS.theta_s * grdROMS.s_w)) / (np.cosh(grdROMS.theta_s) - c1)\n",
    "\n",
    "    else:\n",
    "        Csur = -grdROMS.s_w**2\n",
    "\n",
    "    if (grdROMS.theta_b > 0):\n",
    "        Cbot = (np.exp(grdROMS.theta_b * Csur) - c1 ) / (c1 - np.exp(-grdROMS.theta_b))\n",
    "        grdROMS['Cs_w'] = Cbot\n",
    "    else:\n",
    "        grdROMS['Cs_w'] = Csur     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Obtain z_r\n",
    "\n",
    "    z0 = (grdROMS.hc * grdROMS.s_rho + grdROMS.h * grdROMS.Cs_r) / (grdROMS.hc + grdROMS.h)\n",
    "    grdROMS['z_r'] = grdROMS.zeta + (grdROMS.zeta + grdROMS.h) * z0\n",
    "\n",
    "\n",
    "\n",
    "    # Obtain z_w\n",
    "\n",
    "    z0 = (grdROMS.hc * grdROMS.s_w + grdROMS.h * grdROMS.Cs_w) / (grdROMS.hc + grdROMS.h)\n",
    "    grdROMS['z_w'] = grdROMS.zeta + (grdROMS.zeta + grdROMS.h) * z0\n",
    "\n",
    "\n",
    "\n",
    "    # Also ESMF grid is added but probably not needed for VT\n",
    "\n",
    "\n",
    "\n",
    "    grdROMS['L'] = grdROMS.Lp -1\n",
    "    grdROMS['M'] = grdROMS.Mp -1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return grdROMS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d2ad2b7-d60a-4383-8090-8f6574d81936",
   "metadata": {},
   "source": [
    "def laplacefilter(field, threshold, toxi, toeta):\n",
    "    undef = 2.0e+35\n",
    "    tx = 0.9 * undef\n",
    "    critx = 0.01\n",
    "    cor = 1.6\n",
    "    mxs = 10\n",
    "\n",
    "    field = np.where(abs(field) > threshold, undef, field)\n",
    "\n",
    "    field = ex.extrapolate.fill(int(1), int(toxi),\n",
    "                                int(1), int(toeta),\n",
    "                                float(tx), float(critx), float(cor), float(mxs),\n",
    "                                np.asarray(field, order='F'),\n",
    "                                int(toxi),\n",
    "                                int(toeta))\n",
    "    return field"
   ]
  },
  {
   "cell_type": "raw",
   "id": "447098b1-d0db-4f08-83fb-fcd0b94e3b94",
   "metadata": {},
   "source": [
    "A = np.empty((2,3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b58aa5f7-6b50-40c5-ab7d-a95a2654a1ea",
   "metadata": {},
   "source": [
    "A[0,0] = 2.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68e7a4b0-eb67-4240-8601-7e7524c53a6a",
   "metadata": {},
   "source": [
    "field = data_hr.zos.isel(time = 0)\n",
    "field.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f37b4d47-8073-4406-8948-35af18d1af92",
   "metadata": {},
   "source": [
    "new_field = field.copy()\n",
    "new_f = np.asarray(new_field)\n",
    "undef = 2.0e+35\n",
    "\n",
    "\n",
    "new_f = np.where(abs(field) > 1000, undef, new_f)\n",
    "\n",
    "new_field.values = new_f\n",
    "new_field.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e377efe0-a94c-4763-b792-e896ae31e1fa",
   "metadata": {},
   "source": [
    "# Necessary variables\n",
    "grdMODEL = create_grdMODEL(data_hr)\n",
    "grdROMS = create_grdROMS(grid)\n",
    "\n",
    "field = data_hr.zos.isel(time = 0)\n",
    "toxi = grdROMS.xi_rho_\n",
    "toeta = grdROMS.eta_rho_\n",
    "mymask = grdROMS.mask_rho\n",
    "\n",
    "i1 = 1\n",
    "i2 = int(toxi)              # i1, i2 is the subarea in x direction to be filled\n",
    "j1 = 1         \n",
    "j2 = int(toeta)             # j1, j2 is the subarea in y direction to be filled\n",
    "undef = 2.0e+35\n",
    "tx = 0.9 * undef            # All values in field greater than Tx are filled (real)\n",
    "critx = 0.01                # Criteria for relaxation (DEL**2 = CRIT), usually 4 orders of magnitude down from values in field\n",
    "cor = 1.6                   # Coefficient of overrelaxation, between +1.2 and 2.0\n",
    "mxs = 10                    # Max allowed number of scans in relaxation procedure\n",
    "za = np.asarray(field)      \n",
    "nx = toxi                   # Field x dimension\n",
    "ny = toeta                  # Field y dimension\n",
    "\n",
    "\n",
    "# Necessary data arrays\n",
    "za = np.empty((j2, i2))\n",
    "rmask = np.empty((j2, i2))  # Work array\n",
    "error = np.empty((j2, i2))  # Array containing the errors in the relaxation procedure\n",
    "\n",
    "n = 0\n",
    "suma = 0.\n",
    "\n",
    "for j in range(j2):\n",
    "    \n",
    "    for i in range(i2):\n",
    "        \n",
    "        if za[j,i] < tx:\n",
    "            \n",
    "            suma += za[j,i]\n",
    "            n += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "nvalue = n                  # Number of grid points with value (possibly 0) .... output\n",
    "\n",
    "if n < 1:\n",
    "    \n",
    "    print(f'Warning: No useful data in the field since all data in the interior were smaller than {tx}')\n",
    "\n",
    "\n",
    "suma = suma / n\n",
    "asuma = 0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(j2):\n",
    "    \n",
    "    rmask1 = []\n",
    "    \n",
    "    for i in range(i2):\n",
    "        \n",
    "        if za[j,i] < tx:\n",
    "            \n",
    "            asuma += abs(za[j,i]-suma)\n",
    "            rmask[j,i] = 0.\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            za[j,i] = suma\n",
    "            rmask[j,i] = 1.\n",
    "            \n",
    "            \n",
    "asuma = asuma / n\n",
    "\n",
    "crit = critx * asuma\n",
    "\n",
    "i1p1 = i1 + 1\n",
    "i2m1 = j2 - 1\n",
    "j1p1 = j1 + 1\n",
    "j2m1 = j2 - 1\n",
    "\n",
    "\n",
    "for j in range(j1p1, j2m1):\n",
    "    \n",
    "    for i in range(i1p1, i2m1):\n",
    "        \n",
    "        rmask[j,i] = cor*rmask[j,i]\n",
    "        \n",
    "        \n",
    "for nnn in range(mxs):\n",
    "    \n",
    "    for j in range(j1p1, j2m1):\n",
    "        \n",
    "        for i in range(i1p1, i2m1):\n",
    "            \n",
    "            error[j,i] = (za[j, i+1] + za[j+1, i] + za[j-1, i]) * 0.25 - za[j,i]\n",
    "            \n",
    "        for i in range(i1p1, i2m1):\n",
    "            \n",
    "            error[j,i] = error[j,i] + za[j, i-1] * 0.25\n",
    "            \n",
    "            za[j,i] = za[j,i] + error[j,i] * rmask[j,i]\n",
    "            \n",
    "            \n",
    "    # Test convergence now and then (slow test loop)\n",
    "\n",
    "    if (nnn < mxs - 5) & (nnn % 10 == 0):\n",
    "        \n",
    "        crtest = crit * cor\n",
    "        nbad = 0\n",
    "        j = j1\n",
    "        \n",
    "        while (nbad == 0) & (j < j2m1):\n",
    "            \n",
    "            j += 1\n",
    "            \n",
    "            for i in range(i1p1, i2m1):\n",
    "                \n",
    "                if abs(error[j, i])*rmask[j,i] > crtest:\n",
    "                    \n",
    "                    nbad = 1\n",
    "                    \n",
    "        \n",
    "        if (nbad == 0): \n",
    "            \n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "        \n",
    "        \n",
    "            for j in range(j1p1, j2m1):\n",
    "\n",
    "                za[j, i1] = za[j, i1] + (za[j, i1p1] - za[j, i1])*rmask[j, i1]\n",
    "                za[j, i2] = za[j, i2] + (za[j, i2m1] - za[j, i2])*rmask[j, i2]\n",
    "\n",
    "            for i in range(i1, i2):\n",
    "\n",
    "                za[j1, i] = za[j1, i] + (za[j1p1, i] - za[j1, i])*rmask[j1, i]\n",
    "                za[j2, i] = za[j2, i] + (za[j2m1, i] - za[j2, i])*rmask[j2, i]\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a6df81d-b7c1-40e3-8f5e-62416bd8e57e",
   "metadata": {},
   "source": [
    "new_arr = "
   ]
  },
  {
   "cell_type": "raw",
   "id": "77734f4f-554b-45be-8d5e-90429319b110",
   "metadata": {},
   "source": [
    "new_field = field.copy()\n",
    "new_field.values = za"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fa8d95c-b733-41a3-8b21-9592987fe6f1",
   "metadata": {},
   "source": [
    "new_field.values = za"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee668303-6500-419e-8c08-101184e73ad6",
   "metadata": {},
   "source": [
    "new_field.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80d5c79c-e4d8-48e3-8201-3424b9cd2eba",
   "metadata": {},
   "source": [
    "field.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "feac4b42-704e-48c1-ad73-698c01922a36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f6d6d860-9daf-499b-b27c-386336cfffad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9f49c028-591f-43a2-a32c-28b7615d4043",
   "metadata": {},
   "source": [
    "import ESMF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e5ab1f1-c1d7-4868-a653-892181211854",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ea24df0b-7833-4a5b-ab87-52ecc90441cb",
   "metadata": {},
   "source": [
    "# Alternative horizontal interpolation based on model2roms\n",
    "\n",
    "# Setup ESMF interpolation weights\n",
    "\n",
    "# Source field src at RHO points\n",
    "fieldSrc_rho = ESMF.Field(grdMODEL.esmfgrid,   \"fieldSrc\", staggerloc=ESMF.Staggerloc.CENTER)   # MISSING: grdMODEL.esmfgrid and grdROMS.esmfgrid\n",
    "\n",
    "# Destination field at RHO, U and V points\n",
    "fieldDst_rho = ESMF.Field(grdROMS.esmfgrid,   \"fieldDst\", staggerloc=ESMF.Staggerloc.CENTER)\n",
    "fieldDst_u   = ESMF.Field(grdROMS.esmfgrid_u, \"fieldDst\", staggerloc=ESMF.Staggerloc.CENTER)\n",
    "fieldDst_v   = ESMF.Field(grdROMS.esmfgrid_v, \"fieldDst\", staggerloc=ESMF.Staggerloc.CENTER)\n",
    "\n",
    "# regridSrc2Dst from RHO to U, V and RHO points\n",
    "regridSrc2Dst_rho = ESMF.Regrid(grdMODEL.fieldSrc_rho, grdROMS.fieldDst_rho, regrid_method = ESMF.RegridMethod.BILINEAR, unmapped_action = ESMF.UnmappedAction.IGNORE)\n",
    "regridSrc2Dst_u   = ESMF.Regrid(grdMODEL.fieldSrc_u,   grdROMS.fieldDst_u,   regrid_method = ESMF.RegridMethod.BILINEAR, unmapped_action = ESMF.UnmappedAction.IGNORE)\n",
    "regridSrc2Dst_v   = ESMF.Regrid(grdMODEL.fieldSrc_v,   grdROMS.fieldDst_v,   regrid_method = ESMF.RegridMethod.BILINEAR, unmapped_action = ESMF.UnmappedAction.IGNORE)\n",
    "\n",
    "\n",
    "# Setup indexes\n",
    "indexROMS_Z_ST = (grdMODEL.nlevels, grdROMS.eta_rho_, grdROMD.xi_rho_)\n",
    "toxi = grdROMS.xi_rho_\n",
    "toeta = grdROMS.eta_rho_\n",
    "mymask = grdROMS.mask_rho\n",
    "\n",
    "depth_levels = grdMODEL.nlevels\n",
    "\n",
    "field = grdROMS.regridSrc2Dst_rho(fieldSrc_rho, fieldDst_rho)\n",
    "\n",
    "field = laplacefilter(field, 1000, toxi, toeta)\n",
    "field = field*mymask"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5b3a363-453c-46b9-acb9-92c044c1c741",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "90c002e3-e534-41be-897b-7862bce6d3df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7dca625-b220-4f4d-983d-aab3a88bcd77",
   "metadata": {},
   "source": [
    "## Understand what will happen in the vertical transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730c4b7-c8af-4c7e-bbc9-8eeaed41f3ad",
   "metadata": {},
   "source": [
    "### Show bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ef402-b412-4c40-a896-d74b5a8b6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathymetry = -grid.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976784d8-1b76-400d-bbc5-c740c49f5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathymetry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61959581-bbca-4b8e-84b8-ba9a4266219a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf58067-b858-41ff-bd2c-de4462c4c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reanalysis data and ROMS input grids\n",
    "grdMODEL = create_grdMODEL(data_hr)\n",
    "grdROMS = create_grdROMS(grid)\n",
    "bathymetry = grdROMS.h\n",
    "zr = grdROMS.z_r\n",
    "zs = grdMODEL.z_r\n",
    "Nroms = grdROMS.nlevels\n",
    "Ndata = grdMODEL.nlevels\n",
    "\n",
    "\n",
    "zr = zr.sortby('s_rho', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a37a5a-0b71-490d-8069-24fd6687c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zr[:,:,-1].where(zr[:,:,-1] < zs[Ndata - 1]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31add5d1-cb89-4e78-be69-03889d76b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For case 1, a ROMS grid depth layer is deeper than the deepest data depth layer. However, the deepest data depth layer is zs[Ndata - 1]={zs[Ndata - 1].values}m\\n whereas the deepest depth in the ROMS grid is zr[:,:,-1].min()={zr[:,:,-1].min()}m.\\n Case 1 will thus not happen.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ad188-1bb2-46a0-8097-9c246cdbb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zr[:,:,0:12].where(zr[:,:,0:12] > zs[0]).plot(col='s_rho', col_wrap = 6, vmin = -5, vmax = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bf9f5-11d0-47b3-9bad-77d97d5ee9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For case 2, a ROMS grid depth layer is shallower than the shallowest data depth layer. The shallowest data depth layers is zs[0]={zs[0].values}m.\\n For the shallowest 10 ROMS grid layers there are locations with depths shallower than the shallowest data depth.\\n Case 2 will thus happen for these situations.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d1c6a-d66c-4343-8a7b-442d49941f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom = -1*bathymetry\n",
    "bottom.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de15153-6f42-4486-9428-531754cbdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430e620-51c2-4cdf-9005-c16f1b02095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = zs.size, figsize = (4,3*zs.size))\n",
    "\n",
    "for k in range(zs.size):\n",
    "    bottom.where(zs[k] < bottom).plot(ax = axes[k])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159bd1a9-484c-45fe-bd30-e7e1130e255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For case 3, a ROMS grid depth layer is deeper than some data depth layer but shallower than the next data depth layer which is below bathymetry.\\n Already for the shallower areas, the shallower (thirds) data depth layers are already below the bathymetry. All deeper data depth layers are below some part of the bathymetry.\\n The deepest data depth layers is almost completely below the bathymetry but for some grid points the bathymetry is still deeper.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f17a4-f91b-4ee7-8a13-06ecc38cef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = -10000 \n",
    "print(f'Case 4 assumes that there are values of the reanalysis data smaller than fill={fill}. However, the smallest value are:\\n {data_hr.min()}. \\n Case 4 will thus not happen.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1f232-e2c4-4da8-b210-92c7321a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Case 5 will happen most of the time where a ROMS depth layer lies in between two data depth layers. A weighted average of both data points is used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1dfe0d-78af-4e9e-9b6c-b3cb3c18f550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97d730dd-eaea-4a21-a3ba-b80f7bdcbc27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform the vertical transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de68cb5-9c90-4759-9eb5-196a5a1ae6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_VT_data(input_data, zr, bathymetry, zs, Nroms, Ndata, fill):\n",
    "    ''' Function to obtain the vertical transformed data for a certain depth layer and grid location of the ROMS grid. A time series is returned. '''\n",
    "    \n",
    "    outdat = np.empty(Nroms)\n",
    "    \n",
    "    \n",
    "    for kc in range(int(Nroms)): # Loop over ROMS depth layers (30)\n",
    "        \n",
    "    \n",
    "        # Case 1: ROMS is deeper than GLORYS. This part searches for deepest good value if ROMS depth is deeper than GLORYS. \n",
    "        # This means that if no value, or only fill_value, is available from GLORYS where ROMS is deepest, the closest value from GLORYS is found by looping upward in the water column.\n",
    "\n",
    "        # Between GLORYS/ORA and ROMS grid, CASE 1 will never happen\n",
    "        if zr[kc] < zs[Ndata - 1]:\n",
    "            \n",
    "            \n",
    "            outdat[kc] = input_data[Ndata - 1]\n",
    "            \n",
    "            \n",
    "            #We do not want to give the deepest depth to be nan\n",
    "            \n",
    "            if not np.isnan(np.sum(input_data)):  \n",
    "\n",
    "                if np.isnan(input_data[Ndata - 1]): \n",
    "                    \n",
    "                    for kT in range(int(Ndata)):\n",
    "\n",
    "                        if not np.isnan(input_data[Ndata - 1 - kT]):\n",
    "\n",
    "                            outdat[kc] = input_data[Ndata - 1 - kT]\n",
    "\n",
    "\n",
    "        # Case 2: ROMS depth layer is shallower than GLORYS depth layer. \n",
    "\n",
    "        elif zr[kc] > zs[0]:   \n",
    "            \n",
    "            \n",
    "            outdat[kc] = input_data[0]\n",
    "            \n",
    "\n",
    "        else:\n",
    "\n",
    "            for kT in range(int(Ndata) - 1): # Do loop between surface and bottom of GLORYS depth layers, - 1 because we also check for the next GLORYS layer each step\n",
    "\n",
    "                # Case 3: ROMS depth layer is deeper than some GLORYS depth layer, but shallower than the next GLORYS layer which is below bottom \n",
    "\n",
    "                if (zr[kc] <= zs[kT]) & (-(bathymetry) > zs[kT + 1]):\n",
    "                    \n",
    "                    \n",
    "                    outdat[kc] = input_data[kT]\n",
    "                    \n",
    "                        \n",
    "                    #We do not want to give the deepest depth a nan value\n",
    "                    \n",
    "                    if not np.isnan(np.sum(input_data)):  \n",
    "\n",
    "                        if np.isnan(input_data[Ndata - 1]): \n",
    "\n",
    "                            print(f'Case 3: deepest depth is nan. kc={kc}')\n",
    "                            for kkT in range(int(Ndata)):\n",
    "\n",
    "                                if not np.isnan(input_data[kT - kkT]):\n",
    "                                    \n",
    "                                    outdat[kc] = input_data[kT - kkT]\n",
    "                \n",
    "                # Case 4: Special case where ROMS layers are much deeper than in reanalysis data\n",
    "                \n",
    "                elif (zr[kc] <= zs[kT]) & np.invert(np.isnan(input_data[kT])) & np.isnan(input_data[kT + 1])  :\n",
    "                    \n",
    "                    outdat[kc] = input_data[kT]\n",
    "\n",
    "            \n",
    "\n",
    "                # Case 5: ROMS layer in between two reanalysis data layers. This is the typical case for most layers.\n",
    "\n",
    "                elif (zr[kc] <= zs[kT]) & (zr[kc] >= zs[kT + 1]) & (-(bathymetry) <= zs[kT + 1]):\n",
    "\n",
    "                    rz2 = abs((zr[kc] - zs[kT + 1]) / (abs(zs[kT + 1]) - abs(zs[kT])))\n",
    "\n",
    "                    rz1 = 1.0 - rz2\n",
    "                    \n",
    "                    res = (rz1 * input_data[kT+1] + rz2 * input_data[kT])\n",
    "                    \n",
    "                    \n",
    "                    outdat[kc] = res\n",
    "                    \n",
    "                    #We do not want to give nan value\n",
    "                    if not np.isnan(np.sum(input_data)):  \n",
    "                            \n",
    "                            if np.isnan(input_data[kT]) or np.isnan(input_data[kT + 1]):\n",
    "                                \n",
    "                                print(f'Case 5: one of the values is nan. kc={kc}')\n",
    "                                for kkT in range(Ndata):\n",
    "                                            \n",
    "                                    if not (np.isnan(input_data[kT-kkT])) & (np.isnan(input_data[kT - kkT + 1])):\n",
    "                                        \n",
    "                                        res = rz1 * input_data[kT + 1 - kkT] + rz2 * input_data[kT - kkT]\n",
    "                                        \n",
    "                                        outdat[kc] = res\n",
    "                                        \n",
    "                              \n",
    "    return np.asarray(outdat)\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vertical_transf(data, grid):\n",
    "    ''' Function to perform the vertical transformation.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create reanalysis data and ROMS input grids\n",
    "    grdMODEL = create_grdMODEL(data)\n",
    "    grdROMS = create_grdROMS(grid)\n",
    "\n",
    "    \n",
    "    data = data.drop(['lat', 'lon']).rename({'zos': 'zeta'})                      \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'===================== Start vertical transformation of variables with rho-coordinates (Duration: {round(time.time() - start_time,2)} seconds) ===================================')\n",
    "    \n",
    "    \n",
    "    # Obtain variables\n",
    "    bathymetry = grdROMS.h\n",
    "    zr = grdROMS.z_r\n",
    "    zs = grdMODEL.z_r\n",
    "    Nroms = grdROMS.nlevels\n",
    "    Ndata = grdMODEL.nlevels\n",
    "    fill = -10000                                \n",
    "\n",
    "    dat = data.copy()\n",
    "\n",
    "    \n",
    "    # Change the name of 'depth' and 's_rho' to 'z'\n",
    "    dat = dat.rename({'depth' : 'z'})\n",
    "    zs = zs.rename({'depth' : 'z'})\n",
    "    zr = zr.rename({'s_rho' : 'z'})\n",
    "\n",
    "    \n",
    "    # Transpose dimensions\n",
    "    zr = zr.transpose('z', 'eta_rho', 'xi_rho')\n",
    "    bathymetry = bathymetry.transpose('eta_rho', 'xi_rho')\n",
    "\n",
    "\n",
    "    # Change the arrangememnt of zr to make sure its ordered from surface to bottom\n",
    "    zr = zr.sortby('z', ascending = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'===================== Start vertical transformation of Temperature (Duration: {round(time.time() - start_time,2)} seconds) ======================================================')\n",
    "    \n",
    "    dat_t = dat.thetao.transpose('z', 'time', 'eta_rho', 'xi_rho')\n",
    "    \n",
    "    \n",
    "    theta_dataarray = xr.apply_ufunc(obtain_VT_data,                                                         # The function that should be executed\n",
    "                             dat_t, zr, bathymetry, zs, Nroms, Ndata, fill,                                  # The arguments the function needs\n",
    "                             input_core_dims=[['z'], ['z'], [], ['z'], [], [], []],                     # The list of core dimensions on each input argument that should not be broadcast\n",
    "                             exclude_dims=set(('z',)), \n",
    "                             output_core_dims = [['z']],\n",
    "                             dask = 'parallelized',\n",
    "                             output_dtypes = [dat_t.dtype],\n",
    "                             vectorize = True)\n",
    "    \n",
    "    theta_dataarray = theta_dataarray.rename('temp').rename({'z':'s_rho'}).assign_coords(s_rho = grid.s_rho.sortby('s_rho', ascending = False))\n",
    "    \n",
    "    \n",
    "    print(f'===================== Start vertical transformation of Salinity (Duration: {round((time.time() - start_time)/60,0)} minutes) =========================================================')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dat_s = dat.so.transpose('z', 'time', 'eta_rho', 'xi_rho')\n",
    "    \n",
    "    \n",
    "    sali_dataarray = xr.apply_ufunc(obtain_VT_data,                                                          # The function that should be executed\n",
    "                             dat_s, zr, bathymetry, zs, Nroms, Ndata, fill,                                  # The arguments the function needs\n",
    "                             input_core_dims=[['z'], ['z'], [], ['z'], [], [], []],                          # The list of core dimensions on each input argument that should not be broadcast\n",
    "                             exclude_dims=set(('z',)), \n",
    "                             output_core_dims = [['z']],\n",
    "                             dask = 'parallelized',\n",
    "                             output_dtypes = [dat_s.dtype],\n",
    "                             vectorize = True)\n",
    "\n",
    "    sali_dataarray = sali_dataarray.rename('salt').rename({'z':'s_rho'}).assign_coords(s_rho = grid.s_rho.sortby('s_rho', ascending = False))\n",
    "    \n",
    "    print(f'===================== Start vertical transformation of Zonal Velocity (Duration: {round((time.time() - start_time)/60,0)} minutes) ===================================================')\n",
    "    \n",
    "    # Change coordinate names such that apply_ufunc works correct\n",
    "    dat_u = dat.uo.rename({'xi_u' : 'xi_rho', 'eta_u' : 'eta_rho'})\n",
    "    \n",
    "    dat_u = dat_u.transpose('z', 'time', 'eta_rho', 'xi_rho')\n",
    "    \n",
    "    # Since xi_u.size = 121 and xi_rho.size = 122, drop last values\n",
    "    bathymetry_u = bathymetry[:, :-1]\n",
    "    zr_u = zr[:, :, :-1]\n",
    "    \n",
    "    zonvel_dataarray = xr.apply_ufunc(obtain_VT_data,                                                        # The function that should be executed\n",
    "                             dat_u, zr_u, bathymetry_u, zs, Nroms, Ndata, fill,                              # The arguments the function needs\n",
    "                             input_core_dims=[['z'], ['z'], [], ['z'], [], [], []],                          # The list of core dimensions on each input argument that should not be broadcast\n",
    "                             exclude_dims=set(('z',)), \n",
    "                             output_core_dims = [['z']],\n",
    "                             dask = 'parallelized',\n",
    "                             output_dtypes = [dat_u.dtype],\n",
    "                             vectorize = True)\n",
    "\n",
    "    zonvel_dataarray = zonvel_dataarray.rename('u').rename({'xi_rho' : 'xi_u', 'eta_rho' : 'eta_u', 'z':'s_rho'}).assign_coords(s_rho = grid.s_rho.sortby('s_rho', ascending = False))\n",
    "    \n",
    "    \n",
    "    print(f'===================== Start vertical transformation of Meridional Velocity (Duration: {round((time.time() - start_time)/60,0)} minutes) ==============================================')\n",
    "    \n",
    "    # Change coordinate names such that apply_ufunc works correct\n",
    "    dat_v = dat.vo.rename({'xi_v' : 'xi_rho', 'eta_v' : 'eta_rho'})\n",
    "    \n",
    "    dat_v = dat_v.transpose('z', 'time', 'eta_rho', 'xi_rho')\n",
    "    \n",
    "    # Since eta_v.size = 109 and eta_rho.size = 110, drop last values\n",
    "    bathymetry_v = bathymetry[:-1, :]\n",
    "    zr_v = zr[:, :-1, :]\n",
    "    \n",
    "    mervel_dataarray = xr.apply_ufunc(obtain_VT_data,                                                        # The function that should be executed\n",
    "                             dat_v, zr_v, bathymetry_v, zs, Nroms, Ndata, fill,                              # The arguments the function needs\n",
    "                             input_core_dims=[['z'], ['z'], [], ['z'], [], [], []],                          # The list of core dimensions on each input argument that should not be broadcast\n",
    "                             exclude_dims=set(('z',)), \n",
    "                             output_core_dims = [['z']],\n",
    "                             dask = 'parallelized',\n",
    "                             output_dtypes = [dat_v.dtype],\n",
    "                             vectorize = True)\n",
    "\n",
    "    \n",
    "    mervel_dataarray = mervel_dataarray.rename('v').rename({'xi_rho' : 'xi_v', 'eta_rho' : 'eta_v', 'z':'s_rho'}).assign_coords(s_rho = grid.s_rho.sortby('s_rho', ascending = False))\n",
    "    \n",
    "    \n",
    "    result = xr.merge([theta_dataarray, sali_dataarray, zonvel_dataarray, mervel_dataarray, data.zeta]).sortby('s_rho', ascending = True)\n",
    "    \n",
    "    \n",
    "    print(f'===================== Finished vertical transformation (Duration: {round((time.time() - start_time)/60,0)} minutes) ==================================================================')\n",
    "    \n",
    "    \n",
    "    result.to_netcdf('/Users/iriskeizer/Documents/ROMS/data/lateral forcing/NorthSea4 ORA20C/data_hr_vt_test.nc')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fadaf1-ebbf-4d6d-af6b-9d8631bf8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hr_vt = vertical_transf(data_hr, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebbbe5-b045-410b-ba1e-5cd939085be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac39dc-188a-492b-a5a1-be2fb5ba9a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c1a27-6c93-4d24-b909-16d2fa0fb199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cca81f-04e5-4cb1-b865-90bbe1c36a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464e82a-1a7a-4f7e-9dbc-4127cf6c16f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "644b7dc1-a3fb-4207-a51f-acc419049f7d",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e0d0f-d3b4-44c1-ace1-f22e463452a7",
   "metadata": {},
   "source": [
    "### Check sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52d8f2-b6e1-4662-9651-3d47f2409ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hr_vt.zeta.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062737a2-a11a-44eb-9384-c4dcdf1b15b0",
   "metadata": {},
   "source": [
    "### Check temperature and salinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f7967-0723-4dbc-a71b-1ba7ce8a5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = data_hr_vt.s_rho.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Plot Temperature maps\n",
    "    \n",
    "    clim_tim.temp.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 0], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}m')\n",
    "    \n",
    "    data_hr_vt.temp.isel(time = time_step, s_rho = i).plot(ax = axes[i, 1], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Horizontally regridded data  \\n time =  {data_hr_vt.time.isel(time=time_step).dt.date}\\n s_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f's_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Salinity maps\n",
    "    \n",
    "    \n",
    "    clim_tim.salt.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 2], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}m')\n",
    "    \n",
    "    data_hr_vt.salt.isel(time = time_step, s_rho = i).plot(ax = axes[i, 3], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Horizontally regridded data  \\n time =  {data_hr_vt.time.isel(time=time_step).dt.date}\\n s_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f's_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7972709-b923-4eb2-823c-9b9848bb431e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85d4010a-a544-46ad-8f50-cfd04c8e8adf",
   "metadata": {},
   "source": [
    "### Check zonal and meridional velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7ed38-367b-4ae5-b70f-c689a251edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = data_hr_vt.s_rho.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Change facecolor\n",
    "    axes[i,0].set_facecolor('lightgray')\n",
    "    axes[i,1].set_facecolor('lightgray')\n",
    "    axes[i,2].set_facecolor('lightgray')\n",
    "    axes[i,3].set_facecolor('lightgray')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Zonal velocity maps\n",
    "    \n",
    "    clim_tim.u.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 0], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    data_hr_vt.u.isel(time = time_step, s_rho = i).plot(ax = axes[i, 1], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Vertically transformed data  \\n time =  {data_hr_vt.time.isel(time=time_step).dt.date}\\n s_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f's_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Meridional Velocity maps\n",
    "    \n",
    "    \n",
    "    clim_tim.v.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 2], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    data_hr_vt.v.isel(time = time_step, s_rho = i).plot(ax = axes[i, 3], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Vertically transformed data  \\n time =  {data_hr_vt.time.isel(time=time_step).dt.date}\\n s_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f's_rho = {data_hr_vt.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b54a68-75aa-407e-bcf1-792e8179e6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a566424-12d2-4817-b2ab-f1195e28139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf2319-9e9c-4dfc-81ef-ad5aa7675302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c77b726-6905-41a5-8ec7-8a96d542253b",
   "metadata": {},
   "source": [
    "## Calculate ubar and vbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f91fb-45ba-42ae-96a8-c8023ca9560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_1D(data, z_w, Nroms):\n",
    "    '''\n",
    "    \n",
    "    Function to perform the vertical integration for one horizontal grid point.\n",
    "    '''\n",
    "    \n",
    "    mom = 0.0\n",
    "    \n",
    "    for kc in range(Nroms):\n",
    "        \n",
    "        mom = mom + data[kc] * abs(z_w[kc + 1] - z_w[kc])\n",
    "    \n",
    "    if abs(z_w[0]) > 0.0:\n",
    "        \n",
    "        mom = mom / abs(z_w[0])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        mom = 0.0\n",
    "        \n",
    "    \n",
    "    return np.asarray(mom)\n",
    "\n",
    "\n",
    "\n",
    "def vert_integrate_mom(data, grid):\n",
    "    '''\n",
    "    Calculate vertically integrated momentum component in eta (UBAR) and xi (VBAR) direction\n",
    "    '''\n",
    "    \n",
    "    print(f'===================== Start integrating vertical momentum  =============================================================')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Obtain data\n",
    "    dat_u = data.u\n",
    "    dat_v = data.v\n",
    "    \n",
    "    # Create reanalysis data and ROMS input grids\n",
    "    grdROMS = create_grdROMS(grid)\n",
    "    \n",
    "    # Obtain variables\n",
    "    zr = grdROMS.z_r\n",
    "    z_w = grdROMS.z_w\n",
    "    Nroms = int(grdROMS.nlevels)\n",
    "    \n",
    "    \n",
    "    # Obtain z_wu\n",
    "    z_wu = 0.5*(z_w + z_w.shift(xi_rho = -1)).rename({'xi_rho' : 'xi_u', 'eta_rho' : 'eta_u', 's_w': 's_rho'})\n",
    "    z_wv = 0.5*(z_w + z_w.shift(eta_rho = -1)).rename({'xi_rho' : 'xi_v', 'eta_rho' : 'eta_v', 's_w': 's_rho'})\n",
    "        \n",
    "\n",
    "    # Since xi_u.size = 121 and xi_rho.size = 122, drop last values\n",
    "    z_wu = z_wu[:, :-1, :]\n",
    "    z_wv = z_wv[:-1, :, :]\n",
    "\n",
    "    print(f'===================== Start calculating UBAR ===========================================================================')\n",
    "    \n",
    "    \n",
    "    # Calculate UBAR\n",
    "    UBAR = xr.apply_ufunc(integrate_1D,\n",
    "                         dat_u, z_wu, Nroms,\n",
    "                         input_core_dims=[['s_rho'], ['s_rho'], []],\n",
    "                         exclude_dims=set(('s_rho',)),\n",
    "                         dask = 'parallelized',\n",
    "                         output_dtypes = [dat_u.dtype],\n",
    "                         vectorize = True)\n",
    "\n",
    "    \n",
    "    print(f'===================== Start calculating VBAR ===========================================================================')\n",
    "    \n",
    "    VBAR = xr.apply_ufunc(integrate_1D,\n",
    "                         dat_v, z_wv, Nroms,\n",
    "                         input_core_dims=[['s_rho'], ['s_rho'], []],\n",
    "                         exclude_dims=set(('s_rho',)),\n",
    "                         dask = 'parallelized',\n",
    "                         output_dtypes = [dat_v.dtype],\n",
    "                         vectorize = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    result = xr.merge([data, UBAR.rename('ubar'), VBAR.rename('vbar')])\n",
    "    \n",
    "    \n",
    "    print(f'===================== Finished calculating vertical momentum ===========================================================')\n",
    "    \n",
    "    \n",
    "    result.to_netcdf('/Users/iriskeizer/Documents/ROMS/data/lateral forcing/NorthSea4 ORA20C/result_test.nc')\n",
    "    \n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13e766-a409-4c39-b7dc-32112580067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vert_integrate_mom(data_hr_vt, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ceeeb8-4431-4251-8fa0-bd51c36058a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_tim.ubar.isel(ocean_time = 0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d09c3a-3a0c-46b0-a6a3-bdb3d465316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 0\n",
    "\n",
    "result.ubar.isel(time = time_step).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092a555-1755-47fd-bc27-d3209c8e8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_tim.ubar.isel(ocean_time = 0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39380c-2445-46c6-be0e-2134f05e2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.vbar.isel(time = time_step).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538d88de-a807-4acd-b308-8c729f3d51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/data/lateral forcing/NorthSea4 ORA20C/result_test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d607a6-58c8-4bc3-ad10-4c5675059cd6",
   "metadata": {},
   "source": [
    "## Change time dimension\n",
    "\n",
    "ROMS expects the time input as seconds since 1948-01-01 00:00:00\n",
    "\n",
    "\n",
    "All numpy.datetime64 objects have base 1970-01-01. Also the forcing files Tim's used have this so I will first try without changing this.\n",
    "\n",
    "However, I should change time to ocean_time, but this can be done when the forcing files are created.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff6b09-759c-4b9b-9c6f-c77c1a4d5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the reference time for datetime timesteps\n",
    "print(np.datetime64(0, 'D'))\n",
    "\n",
    "# Obtain seconds between '1948-01-01T00:00:00' and '1970-01-01T00:00:00'\n",
    "sec_shift = np.datetime64('1948-01-01T00:00:00').astype(int)\n",
    "\n",
    "time = result.time.values.astype(int)/10**9         # Create a time array of seconds since 01-01-1970 (ns->s)\n",
    "ocean_time = time - sec_shift                       # Create a time array of seconds since 01-01-1948"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9184d50-41e8-477d-8359-19afa771db38",
   "metadata": {},
   "source": [
    "### Use river time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74dc42f8-0437-487a-960b-942bc536375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "river = xr.open_dataset('/Users/iriskeizer/Documents/ROMS/timnorthsea8/forcing/Rivers_NorthSea8_smooth013_Dai2014_AnnualCycle_1993-2019.nc')\n",
    "\n",
    "ocean_time = river.river_time[24:84].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155aef7-f805-43b2-88a6-40f35256e9fe",
   "metadata": {},
   "source": [
    "## Change NaN values to ROMS fill value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f9f07c-f928-4091-b46f-3130fefc50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['u'] = result.u.where(grdROMS.mask_u == 1, grdROMS.fillval.data)\n",
    "result['v'] = result.v.where(grdROMS.mask_v == 1, grdROMS.fillval.data)\n",
    "\n",
    "result['salt'] = result.salt.where(grdROMS.mask_rho == 1, grdROMS.fillval.data)\n",
    "result['temp'] = result.temp.where(grdROMS.mask_rho == 1, grdROMS.fillval.data)\n",
    "result['zeta'] = result.zeta.where(grdROMS.mask_rho == 1, grdROMS.fillval.data)\n",
    "\n",
    "result['ubar'] = result.ubar.where(grdROMS.mask_u == 1, grdROMS.fillval.data)\n",
    "result['vbar'] = result.vbar.where(grdROMS.mask_v == 1, grdROMS.fillval.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2ba4e-abb5-4725-a519-83514538bf84",
   "metadata": {},
   "source": [
    "## Check for values larger than 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d2971-9447-46b3-8774-d3de10e0f6c0",
   "metadata": {},
   "source": [
    "No values larger than 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af8e0-06fa-480c-a3f6-211142a94bc2",
   "metadata": {},
   "source": [
    "## Create climatology forcing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e93ef75-bd28-4775-a7a4-f356a78b98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clim = result\n",
    "grdROMS = create_grdROMS(grid)\n",
    "\n",
    "clim = xr.Dataset(data_vars=dict(lon_rho=([\"eta_rho\", \"xi_rho\"], grid.lon_rho.values),\n",
    "                                lat_rho=([\"eta_rho\", \"xi_rho\"], grid.lat_rho.values),\n",
    "                                lon_u=([\"eta_u\", \"xi_u\"], grid.lon_u.values),\n",
    "                                lat_u=([\"eta_u\", \"xi_u\"], grid.lat_u.values),\n",
    "                                lon_v=([\"eta_v\", \"xi_v\"], grid.lon_v.values),\n",
    "                                lat_v=([\"eta_v\", \"xi_v\"], grid.lat_v.values),\n",
    "                                lon_psi=([\"eta_psi\", \"xi_psi\"], grid.lon_psi.values),\n",
    "                                lat_psi=([\"eta_psi\", \"xi_psi\"], grid.lat_psi.values),\n",
    "                                h=([\"eta_rho\", \"xi_rho\"], grid.h.values),\n",
    "                                f=([\"eta_rho\", \"xi_rho\"], grid.f.values),\n",
    "                                pm=([\"eta_rho\", \"xi_rho\"], grid.pm.values),\n",
    "                                pn=([\"eta_rho\", \"xi_rho\"], grid.pn.values),\n",
    "                                Cs_r=([\"s_rho\"], grid.Cs_r.values),\n",
    "                                Cs_w=([\"s_w\"], grid.Cs_w.values),\n",
    "                                hc=([], grid.hc.values),\n",
    "                                Tcline=([], grid.Tcline.values),\n",
    "                                theta_s=([], grid.theta_s.values),\n",
    "                                theta_b=([], grid.theta_b.values),\n",
    "                                angle=([\"eta_rho\", \"xi_rho\"], grid.angle.values),\n",
    "                                z_r=([\"s_rho\", \"eta_rho\", \"xi_rho\"], grdROMS.z_r.transpose(\"s_rho\", \"eta_rho\", \"xi_rho\").values),\n",
    "                                z_w=([\"s_w\", \"eta_rho\", \"xi_rho\"], grdROMS.z_w.transpose(\"s_w\", \"eta_rho\", \"xi_rho\").values),\n",
    "                                u=([\"ocean_time\", \"s_rho\", \"eta_u\", \"xi_u\"], result_clim.u.transpose(\"time\", \"s_rho\", \"eta_u\", \"xi_u\").values),\n",
    "                                v=([\"ocean_time\", \"s_rho\", \"eta_v\", \"xi_v\"], result_clim.v.transpose(\"time\", \"s_rho\", \"eta_v\", \"xi_v\").values),\n",
    "                                salt=([\"ocean_time\", \"s_rho\", \"eta_rho\", \"xi_rho\"], result_clim.salt.transpose(\"time\", \"s_rho\", \"eta_rho\", \"xi_rho\").values),\n",
    "                                temp=([\"ocean_time\", \"s_rho\", \"eta_rho\", \"xi_rho\"], result_clim.temp.transpose(\"time\", \"s_rho\", \"eta_rho\", \"xi_rho\").values),\n",
    "                                zeta=([\"ocean_time\", \"eta_rho\", \"xi_rho\"], result_clim.zeta.values),\n",
    "                                ubar=([\"ocean_time\", \"eta_u\", \"xi_u\"], result_clim.ubar.transpose(\"time\", \"eta_u\", \"xi_u\").values),\n",
    "                                vbar=([\"ocean_time\", \"eta_v\", \"xi_v\"], result_clim.vbar.transpose(\"time\", \"eta_v\", \"xi_v\").values),),\n",
    "                  coords=dict(s_rho=([\"s_rho\"], grid.s_rho.values),\n",
    "                              s_w=([\"s_w\"], grid.s_w.values),\n",
    "                              ocean_time=([\"ocean_time\"], ocean_time),\n",
    "                                  ))\n",
    "\n",
    "\n",
    "# variable attributes\n",
    "clim.lon_rho.attrs = {'long_name' : 'Longitude of RHO-points',\n",
    "                      'units' : 'degree_east',\n",
    "                      'standard_name' : 'longitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lat_rho.attrs = {'long_name' : 'Latitude of RHO-points',\n",
    "                      'units' : 'degree_north',\n",
    "                      'standard_name' : 'latitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lon_u.attrs = {'long_name' : 'Longitude of U-points',\n",
    "                      'units' : 'degree_east',\n",
    "                      'standard_name' : 'longitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lat_u.attrs = {'long_name' : 'Latitude of U-points',\n",
    "                      'units' : 'degree_north',\n",
    "                      'standard_name' : 'latitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lon_v.attrs = {'long_name' : 'Longitude of V-points',\n",
    "                      'units' : 'degree_east',\n",
    "                      'standard_name' : 'longitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lat_v.attrs = {'long_name' : 'Latitude of V-points',\n",
    "                      'units' : 'degree_north',\n",
    "                      'standard_name' : 'latitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lon_psi.attrs = {'long_name' : 'Longitude of PSI-points',\n",
    "                      'units' : 'degree_east',\n",
    "                      'standard_name' : 'longitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.lat_psi.attrs = {'long_name' : 'Latitude of PSI-points',\n",
    "                      'units' : 'degree_north',\n",
    "                      'standard_name' : 'latitude',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.h.attrs = {'long_name' : 'Bathymetry at RHO-points',\n",
    "                      'units' : 'meter',\n",
    "                      'standard_name' : 'bath, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.f.attrs = {'long_name' : 'Coriolis parameter at RHO-points',\n",
    "                      'units' : 'second-1',\n",
    "                      'standard_name' : 'Coriolis, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.pm.attrs = {'long_name' : 'curvilinear coordinate metric in XI',\n",
    "                      'units' : 'meter-1',\n",
    "                      'standard_name' : 'pm, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.pn.attrs = {'long_name' : 'curvilinear coordinate metric in ETA',\n",
    "                      'units' : 'meter-1',\n",
    "                      'standard_name' : 'pn, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.Cs_r.attrs = {'long_name' : 'S-coordinate stretching curves at RHO-points',\n",
    "                   'valid_min' : -1.,\n",
    "                   'valid_max' : 0.,\n",
    "                   'field' : 's_rho, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.Cs_w.attrs = {'long_name' : 'S-coordinate stretching curves at W-points',\n",
    "                   'valid_min' : -1.,\n",
    "                   'valid_max' : 0.,\n",
    "                   'field' : 's_w, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.hc.attrs = {'long_name' : 'S-coordinate parameter, critical depth',\n",
    "                   'units' : 'meter'}\n",
    "clim.Tcline.attrs = {'long_name' : 'S-coordinate surface/bottom layer depth',\n",
    "                   'units' : 'meter'}\n",
    "clim.theta_s.attrs = {'long_name' : 'S-coordinate surface control parameter'}\n",
    "clim.theta_b.attrs = {'long_name' : 'S-coordinate bottom control parameter'}\n",
    "clim.angle.attrs = {'long_name' : 'angle between xi axis and east',\n",
    "                   'units' : 'radian',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.z_r.attrs = {'long_name' : 'Sigma layer to depth matrix',\n",
    "                   'units' : 'meter',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.z_w.attrs = {'long_name' : 'Sigma layer to depth matrix',\n",
    "                   'units' : 'meter',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.u.attrs = {'long_name' : 'u-momentum component',\n",
    "                   'units' : 'meter second-1',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'u-velocity, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.v.attrs = {'long_name' : 'v-momentum component',\n",
    "                   'units' : 'meter second-1',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'v-velocity, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.salt.attrs = {'long_name' : 'salinity',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'salinity, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.temp.attrs = {'long_name' : 'potential temperature',\n",
    "                   'units' : 'Celsius',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'temperature, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.zeta.attrs = {'long_name' : 'sea level',\n",
    "                   'units' : 'meter',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'sea level, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.ubar.attrs = {'long_name' : 'u-2D momentum',\n",
    "                   'units' : 'meter second-1',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'u-2D velocity, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.vbar.attrs = {'long_name' : 'v-2D momentum',\n",
    "                   'units' : 'meter second-1',\n",
    "               'time' : 'ocean_time',\n",
    "               'field' : 'v-2D velocity, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.s_rho.attrs = {'long_name' : 'S-coordinate at RHO-points',\n",
    "                   'valid_min' : -1.,\n",
    "                   'valid_max' : 0.,\n",
    "                   'standard_name' : 'ocean_s_coordinate_g2',\n",
    "                   'formula terms' : 's: s_rho C: Cs_r eta: zeta depth: h depth_c: hc',\n",
    "                   'field' : 's_rho, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.s_w.attrs = {'long_name' : 'S-coordinate at W-points',\n",
    "                   'valid_min' : -1.,\n",
    "                   'valid_max' : 0.,\n",
    "                   'standard_name' : 'ocean_s_coordinate_g2',\n",
    "                   'formula terms' : 's: s_w C: Cs_w eta: zeta depth: h depth_c: hc',\n",
    "                   'field' : 's_w, scalar',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.ocean_time.attrs = {'long_name' : 'seconds since 1948-01-01 00:00:00',\n",
    "                        'field' : 'time, scalar, series',\n",
    "               '_FillValue' : grdROMS.fillval.data}\n",
    "clim.ocean_time.encoding = {'units' : 'seconds since 1948-01-01 00:00:00',\n",
    "                        'calendar' : 'standard'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# global attributes\n",
    "clim.attrs = {'title' : 'Climatology forcing file (CLIM) used for forcing the ROMS model',\n",
    "             'description' : 'Created for grid file: NorthSea4_smooth01_sponge_nudg.nc',\n",
    "             'grd_file' : 'Gridfile: .../NorthSea4_smooth01_sponge_nudg.nc',\n",
    "             'history' : f'Created {datetime.date.today().strftime(\"%B %d, %Y\")}',\n",
    "             'conventions' : 'CF-1.0'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0629ca-871d-402f-9b4c-8a8954d6fc95",
   "metadata": {},
   "source": [
    "## Create initial forcing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2cc2fe-8639-4db9-a3cd-9cc22c3fd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = clim.isel(ocean_time = [0])\n",
    "\n",
    "\n",
    "# Change some variables\n",
    "init['Cs_rho'] = init.Cs_r\n",
    "\n",
    "# Drop some variables\n",
    "init = init.drop(['Cs_r', 'Cs_w', 'pm', 'pn', 'f'])\n",
    "\n",
    "\n",
    "# global attributes\n",
    "init.attrs = {'title' : 'Initial forcing file (INIT) used for forcing the ROMS model',\n",
    "             'description' : 'Created for grid file: NORTH_SEA4',\n",
    "             'grd_file' : 'Gridfile: .../NorthSea4_smooth01_sponge_nudg.nc',\n",
    "             'history' : f'Created {datetime.date.today().strftime(\"%B %d, %Y\")}'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5208f3-c092-403d-b84f-de7451fe0441",
   "metadata": {},
   "source": [
    "## Create boundary forcing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a40174-6000-4d15-9263-3bdbce7672e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bry = clim.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Assign lon_rho and lat_rho as coordinates, not as variables\n",
    "bry.assign_coords(lon_rho = bry.lon_rho, lat_rho = bry.lat_rho)\n",
    "\n",
    "\n",
    "\n",
    "# Create boundary variables\n",
    "bry['temp_west'] = bry.temp.isel(xi_rho = 0)\n",
    "bry['temp_east'] = bry.temp.isel(xi_rho = -1)\n",
    "bry['temp_south'] = bry.temp.isel(eta_rho = 0)\n",
    "bry['temp_north'] = bry.temp.isel(eta_rho = -1)\n",
    "\n",
    "bry['salt_west'] = bry.salt.isel(xi_rho = 0)\n",
    "bry['salt_east'] = bry.salt.isel(xi_rho = -1)\n",
    "bry['salt_south'] = bry.salt.isel(eta_rho = 0)\n",
    "bry['salt_north'] = bry.salt.isel(eta_rho = -1)\n",
    "\n",
    "bry['zeta_west'] = bry.zeta.isel(xi_rho = 0)\n",
    "bry['zeta_east'] = bry.zeta.isel(xi_rho = -1)\n",
    "bry['zeta_south'] = bry.zeta.isel(eta_rho = 0)\n",
    "bry['zeta_north'] = bry.zeta.isel(eta_rho = -1)\n",
    "\n",
    "bry['u_west'] = bry.u.isel(xi_u = 0)\n",
    "bry['u_east'] = bry.u.isel(xi_u = -1)\n",
    "bry['u_south'] = bry.u.isel(eta_u = 0)\n",
    "bry['u_north'] = bry.u.isel(eta_u = -1)\n",
    "\n",
    "bry['v_west'] = bry.v.isel(xi_v = 0)\n",
    "bry['v_east'] = bry.v.isel(xi_v = -1)\n",
    "bry['v_south'] = bry.v.isel(eta_v = 0)\n",
    "bry['v_north'] = bry.v.isel(eta_v = -1)\n",
    "\n",
    "bry['ubar_west'] = bry.ubar.isel(xi_u = 0)\n",
    "bry['ubar_east'] = bry.ubar.isel(xi_u = -1)\n",
    "bry['ubar_south'] = bry.ubar.isel(eta_u = 0)\n",
    "bry['ubar_north'] = bry.ubar.isel(eta_u = -1)\n",
    "\n",
    "bry['vbar_west'] = bry.vbar.isel(xi_v = 0)\n",
    "bry['vbar_east'] = bry.vbar.isel(xi_v = -1)\n",
    "bry['vbar_south'] = bry.vbar.isel(eta_v = 0)\n",
    "bry['vbar_north'] = bry.vbar.isel(eta_v = -1)\n",
    "\n",
    "\n",
    "'''\n",
    "# Assign attributes\n",
    "bry.lon_rho.attrs = {'long_name' : 'Longitude of RHO-points',\n",
    "                      'units' : 'degree_east',\n",
    "                      'standard_name' : 'longitude'}\n",
    "bry.lat_rho.attrs = {'long_name' : 'Latitude of RHO-points',\n",
    "                      'units' : 'degree_north',\n",
    "                      'standard_name' : 'latitude'}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Drop some variables\n",
    "bry = bry.drop(['lon_rho', 'lat_rho', 'f', 'pm', 'pn', 'z_w', 'temp', 'salt', 'zeta', 'u', 'v', 'ubar', 'vbar'])\n",
    "\n",
    "\n",
    "\n",
    "# global attributes\n",
    "bry.attrs = {'title' : 'Boundary forcing file (BRY) used for forcing the ROMS model',\n",
    "             'description' : 'Created for grid file: NORTH_SEA4',\n",
    "             'grd_file' : 'Gridfile: .../NorthSea4_smooth01_sponge_nudg.nc',\n",
    "             'history' : f'Created {datetime.date.today().strftime(\"%B %d, %Y\")}'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa13ea2-7670-4550-80a0-bfdab9d88f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d71b24-5889-4e02-b1a9-461ecd5a810c",
   "metadata": {},
   "source": [
    "## Save the forcing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0add358-1384-439f-b389-4879f6d62432",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/Users/iriskeizer/Documents/ROMS/data/lateral forcing/NorthSea4 {data_type}'\n",
    "#start_date = f'{clim.ocean_time[0].dt.year.data}{clim.ocean_time[0].dt.month.data}{clim.ocean_time[0].dt.day.data}'\n",
    "#final_date = f'{clim.ocean_time[-1].dt.year.data}{clim.ocean_time[-1].dt.month.data}{clim.ocean_time[-1].dt.day.data}'\n",
    "\n",
    "\n",
    "#init.to_netcdf(f'{path}/NorthSea4_test_init_{data_type}_{start_date}_to_{final_date}.nc')\n",
    "#bry.to_netcdf(f'{path}/NorthSea4_test_bry_{data_type}_{start_date}_to_{final_date}.nc')\n",
    "#clim.to_netcdf(f'{path}/NorthSea4_test_clim_{data_type}_{start_date}_to_{final_date}.nc')\n",
    "\n",
    "init.to_netcdf(f'{path}/NorthSea4_test_init_ORA20C_1950116_to_19541216.nc')\n",
    "bry.to_netcdf(f'{path}/NorthSea4_test_bry_ORA20C_1950116_to_19541216.nc')\n",
    "clim.to_netcdf(f'{path}/NorthSea4_test_clim_ORA20C_1950116_to_19541216.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d02f7-4e44-479c-b196-0a1796ad65dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e55d2-e5b4-4f5e-95e5-e382e9195fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47ca0937-39ec-4319-bcfd-e5d13a35b314",
   "metadata": {},
   "source": [
    "# Compare forcing files to Tim's forcing files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d8d08-8e4f-4f39-bf9e-15f594c7b16e",
   "metadata": {},
   "source": [
    "## Compare climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c93b16-d05f-4c79-ad0d-f0eabdb8abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change grdROMS fill values back to nan\n",
    "\n",
    "clim['u'] = clim.u.where(grdROMS.mask_u == 1, np.nan)\n",
    "clim['v'] = clim.v.where(grdROMS.mask_v == 1, np.nan)\n",
    "\n",
    "clim['salt'] = clim.salt.where(grdROMS.mask_rho == 1, np.nan)\n",
    "clim['temp'] = clim.temp.where(grdROMS.mask_rho == 1, np.nan)\n",
    "clim['zeta'] = clim.zeta.where(grdROMS.mask_rho == 1, np.nan)\n",
    "\n",
    "clim['ubar'] = clim.ubar.where(grdROMS.mask_u == 1, np.nan)\n",
    "clim['vbar'] = clim.vbar.where(grdROMS.mask_v == 1, np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260fe72-b220-4990-bcc2-7299ca09b7c5",
   "metadata": {},
   "source": [
    "### Sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98633a-ab58-4aa8-8faa-5795da9b24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = clim.ocean_time.size\n",
    "fig, axes = plt.subplots(ncols = 2, nrows = nrows, figsize = (10, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Change facecolor\n",
    "    axes[i,0].set_facecolor('lightgray')\n",
    "    axes[i,1].set_facecolor('lightgray')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Sea Level maps\n",
    "    \n",
    "    clim_tim.zeta.isel(ocean_time = i).plot(ax = axes[i, 0], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"Sea level [m]\"})\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f'time = {clim_tim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    clim.zeta.isel(ocean_time = i).plot(ax = axes[i, 1], vmin = -0.7, vmax = 0.7, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"Sea level [m]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f'time = {clim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('comparison_clim_SL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f9477-54c7-4c95-bc34-8a7d57a6a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadc44f9-2fad-4bde-9ee0-072d4b10d845",
   "metadata": {},
   "source": [
    "### Temperature and Salinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0dda9-c120-4bf8-8a18-b0216648402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = -1\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = clim.s_rho.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Plot Temperature maps\n",
    "    \n",
    "    clim_tim.temp.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 0], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}m')\n",
    "    \n",
    "    clim.temp.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 1], vmin = -2, vmax = 20, cbar_kwargs={\"label\": \"Temperature [째C]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f's_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Salinity maps\n",
    "    \n",
    "    \n",
    "    clim_tim.salt.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 2], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}m')\n",
    "    \n",
    "    clim.salt.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 3], vmin = 30, vmax = 40, cbar_kwargs={\"label\": \"Salinity [1e-3]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f's_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig('comparison_clim_TS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18390145-8ccf-403a-93fd-95cbb1e803c7",
   "metadata": {},
   "source": [
    "### Zonal and Meridional Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb93f05-8cba-4d63-91d5-03548cf50d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "time_step = 0\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = clim.s_rho.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Change facecolor\n",
    "    axes[i,0].set_facecolor('lightgray')\n",
    "    axes[i,1].set_facecolor('lightgray')\n",
    "    axes[i,2].set_facecolor('lightgray')\n",
    "    axes[i,3].set_facecolor('lightgray')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Zonal velocity maps\n",
    "    \n",
    "    clim_tim.u.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 0], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    clim.u.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 1], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f's_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Meridional Velocity maps\n",
    "    \n",
    "    \n",
    "    clim_tim.v.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 2], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f's_rho = {clim_tim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    clim.v.isel(ocean_time = time_step, s_rho = i).plot(ax = axes[i, 3], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=time_step).dt.date}\\n s_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f's_rho = {clim.s_rho.isel(s_rho = i)}')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('comparison_clim_UV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a1e57-a3db-47b9-b592-1bd503c2ae5e",
   "metadata": {},
   "source": [
    "### Zonal and Meridional momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7a85e-8e17-4d96-9d6b-de09fdf893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "clim_tim = clim_tim.where()\n",
    "\n",
    "\n",
    "# Create figures\n",
    "nrows = clim.ocean_time.size\n",
    "fig, axes = plt.subplots(ncols = 4, nrows = nrows, figsize = (14, 2.7*nrows))\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    # Change facecolor\n",
    "    axes[i,0].set_facecolor('lightgray')\n",
    "    axes[i,1].set_facecolor('lightgray')\n",
    "    axes[i,2].set_facecolor('lightgray')\n",
    "    axes[i,3].set_facecolor('lightgray')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Zonal velocity maps\n",
    "    \n",
    "    clim_tim.ubar.isel(ocean_time = i).plot(ax = axes[i, 0], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U 2D momentum [m/s]\"})\n",
    "    \n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 0].set_title(f'time = {clim_tim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    clim.ubar.isel(ocean_time = i).plot(ax = axes[i, 1], vmin = -0.3, vmax = 0.3, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"U 2D momentum [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 1].set_title(f'time = {clim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot Meridional Velocity maps\n",
    "    \n",
    "    \n",
    "    clim_tim.vbar.isel(ocean_time = i).plot(ax = axes[i, 2], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V 2D momentum [m/s]\"})\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i, 2].set_title(f'Climatology forcing file Tim\\n time =  {clim_tim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 2].set_title(f'time = {clim_tim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    clim.vbar.isel(ocean_time = i).plot(ax = axes[i, 3], vmin = -0.15, vmax = 0.15, cmap = 'RdBu_r', cbar_kwargs={\"label\": \"V 2D momentum [m/s]\"})\n",
    "   \n",
    "    if i == 0:\n",
    "        axes[i, 3].set_title(f'Climatology forcing file Iris\\n time =  {clim.ocean_time.isel(ocean_time=i).dt.date}')\n",
    "    else:\n",
    "        axes[i, 3].set_title(f'time = {clim.ocean_time.isel(ocean_time = i).dt.date}')\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('comparison_clim_UV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145ef3a-d1ff-4960-bc0e-02fccd8934b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmf",
   "language": "python",
   "name": "esmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
